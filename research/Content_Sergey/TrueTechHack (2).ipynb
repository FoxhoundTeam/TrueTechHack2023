{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mblRbfsItsUh",
        "outputId": "9c384ded-1a44-4b03-ccd3-7a73cc4a1a71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nsfw_model'...\n",
            "remote: Enumerating objects: 487, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 487 (delta 36), reused 23 (delta 9), pack-reused 397\u001b[K\n",
            "Receiving objects: 100% (487/487), 472.34 KiB | 5.62 MiB/s, done.\n",
            "Resolving deltas: 100% (242/242), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/GantMan/nsfw_model.git  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFjzn4PFCNHp",
        "outputId": "7739772e-1222-4fb9-9ab5-99b167b34a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytube\n",
            "  Downloading pytube-12.1.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 KB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-12.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pytube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMzPdgKf80rx",
        "outputId": "f1ab5954-f048-44d8-c283-0ef3c040ff17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scenedetect[opencv]\n",
            "  Downloading scenedetect-0.6.1-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (4.65.0)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (8.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (1.22.4)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (1.4.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from scenedetect[opencv]) (4.7.0.72)\n",
            "Installing collected packages: scenedetect\n",
            "Successfully installed scenedetect-0.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade scenedetect[opencv]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8I4yL3zU597r",
        "outputId": "eb9957bc-1663-46bb-98f0-f1e6c47c79dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-03-28 20:07:26--  https://s3.amazonaws.com/ir_public/ai/nsfw_models/nsfw.299x299.h5\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.36.128, 52.217.164.80, 52.216.219.104, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.36.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 158652512 (151M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘nsfw.299x299.h5’\n",
            "\n",
            "nsfw.299x299.h5     100%[===================>] 151.30M  32.2MB/s    in 5.3s    \n",
            "\n",
            "2023-03-28 20:07:32 (28.5 MB/s) - ‘nsfw.299x299.h5’ saved [158652512/158652512]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3.amazonaws.com/ir_public/ai/nsfw_models/nsfw.299x299.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQBS_-99Aubd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHgpDr115JBj"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "PATH_TO_NSWF_MODEL = \"/content/nsfw_model\"\n",
        "if PATH_TO_NSWF_MODEL not in sys.path:\n",
        "    sys.path.append(PATH_TO_NSWF_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8kDN5PxNX6n"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def download_video_from_youtube(urls, dir, file_extension=\"mp4\", resolution=\"360p\"):\n",
        "    if isinstance(urls, str):\n",
        "        urls = [urls]\n",
        "    for url in tqdm(urls):\n",
        "        yt = YouTube(url)\n",
        "        stream = yt.streams.filter(file_extension=file_extension)\n",
        "        stream.get_by_resolution(resolution).download(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZmxI3DKO4Xg",
        "outputId": "5d49194c-c74b-4b36-a296-4f4e60e0b1dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.91s/it]\n"
          ]
        }
      ],
      "source": [
        "urls = \"https://www.youtube.com/watch?v=_xGuLjpdmNM\"\n",
        "download_video_from_youtube(urls, \"/content/\", resolution=\"360p\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "5314SQh7CwbM",
        "outputId": "e7909bdc-2b6b-482f-ae7b-d9d0c236c4e9"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cdd6a4d56d4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mabc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mABC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnsfw_detector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscenedetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mContentDetector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdaptiveDetector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nsfw_detector'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from abc import ABC, abstractmethod\n",
        "from nsfw_detector import predict\n",
        "from scenedetect import detect, ContentDetector, AdaptiveDetector\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kP4UU_UG0dTA"
      },
      "outputs": [],
      "source": [
        "def central_crop(image):\n",
        "    center = image.shape\n",
        "    w = h = min(image.shape[:2])\n",
        "    x = center[1]/2 - w/2\n",
        "    y = center[0]/2 - h/2\n",
        "\n",
        "    crop_img = image[int(y):int(y+h), int(x):int(x+w)]\n",
        "    return crop_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgpX1VSMUtdU"
      },
      "outputs": [],
      "source": [
        "class IntimateVideoClassifier:\n",
        "    \"\"\"NSWF classifier class.\n",
        "\n",
        "    Attributes:\n",
        "    * model_path - path to the model weights\n",
        "    * input_dim - dimension of input for model.\n",
        "    * batch_size - is a batch size of frames to the model, classify \n",
        "    frames when they will gather in batch.\n",
        "    * threshold - is model threshold (prob_of_iclass > threshold => iclass).\n",
        "\n",
        "    Methods:\n",
        "    * predict - returns probs of frames.\n",
        "    * classify_scenes - return timecode and frames of scenes to censor\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_path, input_dim, batch_size, threshold=.8):\n",
        "        \"\"\"Note. input_dim = according to the weights of the model 224x224 or 299x299.\"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.input_dim = input_dim\n",
        "        self.threshold = threshold\n",
        "        self.model = predict.load_model(model_path)\n",
        "        self.central_crop = True\n",
        "\n",
        "    def predict(self, video_path, start_frame=0, end_frame=None, classify_every_n_frames=1):\n",
        "        \"\"\"Return probs of frames nswf classes starting from \n",
        "        `start_frame`(inclusive) to `end_frame`(exclusive) \n",
        "        with step `classify_every_n_frames`.\n",
        "\n",
        "        Example: {'number_of_frame':{'class':probability, ...}, ...}\n",
        "        \"\"\"\n",
        "        frame_count = start_frame\n",
        "\n",
        "        batch = []\n",
        "        frames_idx = []\n",
        "        output = {}\n",
        "\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        # end frame or video length\n",
        "        end_frame = end_frame or int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        # set start frame\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "        while cap.isOpened() and (frame_count < end_frame):\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                frame = self._preprocess(frame)\n",
        "                # collect frames to batch with indexes\n",
        "                frames_idx.append(frame_count)\n",
        "                batch.append(frame)\n",
        "                # process batch\n",
        "                if len(batch) >= self.batch_size:\n",
        "                    minibatch = np.concatenate(batch, axis=0)\n",
        "                    probs = predict.classify_nd(self.model, minibatch)\n",
        "                    output.update(dict(zip(frames_idx, probs)))\n",
        "                    # clear frames and indexes collections\n",
        "                    batch.clear()\n",
        "                    frames_idx.clear()\n",
        "\n",
        "                frame_count += classify_every_n_frames\n",
        "                # move forward for `classify_every_n_frames` frames\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_count)\n",
        "            else:\n",
        "                cap.release()\n",
        "                break\n",
        "\n",
        "        if len(batch) > 0:\n",
        "            minibatch = np.concatenate(batch, axis=0)\n",
        "            probs = predict.classify_nd(self.model, minibatch)\n",
        "            output.update(dict(zip(frames_idx, probs)))\n",
        "\n",
        "        return output\n",
        "\n",
        "    def classify_scenes(self, video_path, scenes=None, scene_threshold=.1, classify_every_n_frames=1):\n",
        "        \"\"\"Return List of tuples (start of the scene: FrameTimecode, end of the scene: FrameTimecode)\n",
        "        \n",
        "        Args:\n",
        "        * scenes - List of tuples of FrameTimecode to be classified. \n",
        "        If None than create with alghorithm. default None.\n",
        "        * scene_threshold - if number of scenes with nswf class > scene_threshold than add it to return.\n",
        "\n",
        "        FrameTimecode. https://scenedetect.com/projects/Manual/en/latest/api/frame_timecode.html#scenedetect-frame-timecode\n",
        "        \"\"\"\n",
        "        scenes = scenes or detect(video_path, ContentDetector())\n",
        "        nswf_scenes = []\n",
        "        # TODO We create stream for every scene\n",
        "        for scene_start, scene_end in scenes:\n",
        "            scene_start_frame = scene_start.get_frames()\n",
        "            scene_end_frame = scene_end.get_frames()\n",
        "            scene_length = (scene_end_frame - scene_start_frame) / classify_every_n_frames\n",
        "\n",
        "            # TODO Highlight slow movments and blurry frames\n",
        "            probs = self.predict(video_path, \n",
        "                         start_frame=scene_start_frame, \n",
        "                         end_frame=scene_end_frame,\n",
        "                         classify_every_n_frames=classify_every_n_frames\n",
        "                         )\n",
        "            nswf_scenes_count = sum([\n",
        "                prob[\"porn\"] > self.threshold \n",
        "                or prob[\"hentai\"] > self.threshold \n",
        "                or prob[\"sexy\"] > self.threshold \n",
        "                for prob in probs.values()\n",
        "            ])\n",
        "            # print(scene_start_frame, scene_end_frame, \":\", (nswf_scenes_count / scene_length))\n",
        "            if (nswf_scenes_count / scene_length) > scene_threshold:\n",
        "                nswf_scenes.append((scene_start, scene_end))\n",
        "        return self.gluing(nswf_scenes)\n",
        "\n",
        "    @classmethod\n",
        "    def gluing(cls, raw_nswf_scenes):\n",
        "        \"\"\"Glue close frames.\"\"\"\n",
        "        nswf_scenes = []\n",
        "        for idx in range(len(raw_nswf_scenes)):\n",
        "            if len(nswf_scenes) > 0:\n",
        "                pred_end = nswf_scenes[-1][1].get_frames()\n",
        "                cur_start = raw_nswf_scenes[idx][0].get_frames()\n",
        "                if (cur_start - pred_end) < 20:\n",
        "                    nswf_scenes[-1] = (nswf_scenes[-1][0], raw_nswf_scenes[idx][1])\n",
        "                else:\n",
        "                    nswf_scenes.append(raw_nswf_scenes[idx])\n",
        "            else:    \n",
        "                nswf_scenes.append(raw_nswf_scenes[idx])\n",
        "        return nswf_scenes\n",
        "\n",
        "    def _preprocess(self, frame):\n",
        "        frame = central_crop(frame) if self.central_crop else frame\n",
        "        resized_frame = cv2.resize(frame, (self.input_dim, self.input_dim))\n",
        "        resized_frame = resized_frame / 255\n",
        "        return resized_frame[np.newaxis, ...]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMD5198Ncth2"
      },
      "outputs": [],
      "source": [
        "classifier = IntimateVideoClassifier(\"/content/nsfw.299x299.h5\", 299, 24)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tANfT3L2iwAW"
      },
      "outputs": [],
      "source": [
        "video_path = \"/content/Глюк’оZа - Мотыльки (feat KYIVSTONER).mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1iroKWndEN3",
        "outputId": "2bdd89d4-e90d-40e1-eebe-c6687ad87599"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pyscenedetect:Downscale factor set to 2, effective resolution: 320 x 180\n",
            "INFO:pyscenedetect:Detecting scenes...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 52ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 48ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 66ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 56ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 69ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 47ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 55ms/step\n",
            "1/1 [==============================] - 0s 51ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 60ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-173-5434e96da377>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_scenes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscene_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassify_every_n_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/video_classifier.py\u001b[0m in \u001b[0;36mclassify_scenes\u001b[0;34m(self, video_path, scenes, scene_threshold, classify_every_n_frames)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# TODO Highlight slow movments and blurry frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             probs = self.predict(video_path, \n\u001b[0m\u001b[1;32m    139\u001b[0m                          \u001b[0mstart_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscene_start_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                          \u001b[0mend_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscene_end_frame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/video_classifier.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, video_path, start_frame, end_frame, classify_every_n_frames)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mframe_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclassify_every_n_frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# move forward for `classify_every_n_frames` frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "out = classifier.classify_scenes(video_path, scene_threshold=.3, classify_every_n_frames=1)\n",
        "out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAJDEdg1r49o"
      },
      "outputs": [],
      "source": [
        "def cut_video_fragment(video_path, fragment_path, start_frame=0, end_frame=None):\n",
        "    frame_count = start_frame\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "    out = cv2.VideoWriter(fragment_path, fourcc, 20.0, (width,  height))\n",
        "    # end frame or video length\n",
        "    end_frame = end_frame or int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    # set start frame\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
        "    while cap.isOpened() and (frame_count < end_frame):\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            out.write(frame)\n",
        "            frame_count += 1\n",
        "        else:\n",
        "            cap.release()\n",
        "            out.release()\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nJ8KGsbwhhL"
      },
      "outputs": [],
      "source": [
        "for idx, (start, end) in enumerate(out):\n",
        "    srart = start.get_frames()\n",
        "    end = end.get_frames()\n",
        "    cut_video_fragment(video_path, f\"{video_path[:-4]}-{idx}.mp4\", srart, end)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCQay6fZKo6R",
        "outputId": "885e98ca-6c93-456c-e6b2-effedac72812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.0.58-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.8/486.8 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thop>=0.1.1\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from ultralytics) (5.9.4)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.14.1+cu116)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.65.0)\n",
            "Collecting sentry-sdk\n",
            "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (4.7.0.72)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.4.4)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (2.27.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (0.12.2)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (8.4.0)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.13.1+cu116)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (6.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (4.39.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.2.2->ultralytics) (1.0.7)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.4->ultralytics) (2022.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.23.0->ultralytics) (2.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7.0->ultralytics) (4.5.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
            "Installing collected packages: sentry-sdk, thop, ultralytics\n",
            "Successfully installed sentry-sdk-1.18.0 thop-0.1.1.post2209072238 ultralytics-8.0.58\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcpbR2_2DWd0"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "class SmokersVideoDetector:\n",
        "    \"\"\"Smokers detector class.\n",
        "\n",
        "    Attributes:\n",
        "    * model_path - path to the model weights\n",
        "    * threshold - is model threshold (prob_of_ibbox > threshold => ibbox).\n",
        "\n",
        "    Methods:\n",
        "    * predict - returns probs of frames.\n",
        "    * detect - returns bboxes with corisponding classes to every frame in video.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_path):\n",
        "        self.model = YOLO(model_path)\n",
        "\n",
        "    def predict(self, video_path):\n",
        "        \"\"\"TODO: Return probs of frames nswf classes starting from \n",
        "        `start_frame`(inclusive) to `end_frame`(exclusive) \n",
        "        with step `classify_every_n_frames`.\n",
        "\n",
        "        Example: {'number_of_frame':{'class':probability, ...}, ...}\n",
        "        \"\"\"\n",
        "        return self.model(video_path)\n",
        "\n",
        "    def detect(self, video_path, threshold=.1):\n",
        "        \"\"\"Return List of tuples (start of the scene: FrameTimecode, end of the scene: FrameTimecode)\n",
        "        \n",
        "        Args:\n",
        "        * scenes - List of tuples of FrameTimecode to be classified. \n",
        "        If None than create with alghorithm. default None.\n",
        "        * scene_threshold - if number of scenes with nswf class > scene_threshold than add it to return.\n",
        "\n",
        "        FrameTimecode. https://scenedetect.com/projects/Manual/en/latest/api/frame_timecode.html#scenedetect-frame-timecode\n",
        "        \"\"\"\n",
        "        resutls = self.predict(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c7UQdn9pxkXQ"
      },
      "outputs": [],
      "source": [
        "model = SmokersVideoDetector(\"/content/drive/MyDrive/Colab Notebooks/data/runs/detect/train4/weights/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OX6NtBiU2Gpm"
      },
      "outputs": [],
      "source": [
        "results = model.predict(\"/content/Beautiful Smoking Scenes in Films.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLWvSkdf2OSF",
        "outputId": "91348e36-6379-485e-8806-d537d6998d32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bbox-visualizer\n",
            "  Downloading bbox_visualizer-0.1.0-py2.py3-none-any.whl (6.2 kB)\n",
            "Installing collected packages: bbox-visualizer\n",
            "Successfully installed bbox-visualizer-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bbox-visualizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wT7_wt35RSJ",
        "outputId": "d4dba3da-7cb9-4702-a503-63496db355ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[     6.0165,           0,       636.6,         296]], dtype=float32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[15].cpu().numpy().boxes.xyxy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liBZqEYz88s8",
        "outputId": "2e18b8a7-2add-4425-dd30-06a47c76946d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'cigarette', 1: 'person', 2: 'smoke'}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results[1].cpu().numpy().names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjK8lec73K9X",
        "outputId": "43c26744-30ef-4626-882c-0cd8191efecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        }
      ],
      "source": [
        "import bbox_visualizer as bbv\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "conf_threshold = .8\n",
        "frame_count = 0\n",
        "cap = cv2.VideoCapture(\"/content/Beautiful Smoking Scenes in Films.mp4\")\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
        "out = cv2.VideoWriter(\"/content/out.mp4\", fourcc, 20.0, results[0].orig_shape[::-1])\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        result = results[frame_count].cpu().numpy()\n",
        "        labels_names = result.names\n",
        "        boxes = results[frame_count].cpu().numpy().boxes\n",
        "        # if boxes.shape[0] == 0:\n",
        "        #     frame_count += 1\n",
        "        #     continue\n",
        "        # print(boxes)\n",
        "        # for bbox, conf, cls in zip(boxes.xyxy, boxes.conf, boxes.cls):\n",
        "        #     label = labels_names[cls]\n",
        "        #     if conf > conf_threshold:\n",
        "        #         # frame = bbv.draw_rectangle(frame, np.uint8(bbox))\n",
        "        #         # frame = bbv.add_label(frame, label, np.uint8(bbox), top=True)\n",
        "        #         result.plot()\n",
        "        #         break\n",
        "        out.write(result.plot())\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
